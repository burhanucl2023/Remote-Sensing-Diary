[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Remote Sensing",
    "section": "",
    "text": "Welcome\nI am Burhan. Born and brought up in the beautiful valley of Kashmir.Yeah, we are famous for Beautiful Valleys, Mesmerizing Lakes, Cashmere Shawls & Wazwan (our local cuisine). Take a glimpse of my hometown through this elaborate Photo Gallery or the one short one below.\n\n\n\n\nI am a Passionate Urban Spatial Enthusiast pursuing postgraduate studies at University College London (UCL), blending expertise in Urban Spatial Sciences and Civil Engineering.I have held mid-level management positions in both government and the public sector, specializing in the strategic development of road networks for underserved areas in India. My mojo revolves around crafting the future of transportation design, playing with spatial analytics, rocking disaster management scenarios, championing net-zero strategies, and setting the stage for cool public policies. Committed to driving positive change, seeking collaboration with like-minded professionals to shape a more sustainable future for cities and regions.\n\n \n\nGratitude: As I was away from first five weeks, so had to catch up to a lot in a small amount of time. But, I would like to express my gratitude to the my department (in general) and Dr Andrew MacLachlan (in particular) for guiding me through this and I hope I have managed to produce a good learning diary and met the expectations of my peers."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee (knuth84?) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Ahmad, Dr. Zia. n.d. “Geospatial Data Science in\nR.” Accessed March 9, 2024. https://zia207.github.io/geospatial-r-github.io/index.html.\n\n\nDarbari, Priyanka, and Manoj Kumar. 2022. “Satellite Image\nEnhancement Techniques: A Comprehensive\nReview.” In Proceedings of International\nConference on Communication and Artificial\nIntelligence, edited by Vishal Goyal, Manish Gupta, Seyedali\nMirjalili, and Aditya Trivedi, 431–47. Lecture Notes in\nNetworks and Systems. Singapore: Springer\nNature. https://doi.org/10.1007/978-981-19-0976-4_36.\n\n\nEarthdata-Website. 2019. “What Is Remote Sensing? |\nEarthdata.” Backgrounder. Earth Science Data\nSystems, NASA. August 23, 2019. https://www.earthdata.nasa.gov/learn/backgrounders/remote-sensing.\n\n\nElachi, Charles, and van Zyl Jakob J. 2006. “Introduction to the\nPhysics and Techniques of Remote\nSensing.” In Introduction to the Physics and\nTechniques of Remote Sensing, 1–21. Hoboken, NJ, USA: John Wiley\n& Sons, Inc.\n\n\nGIPHY, dir. n.d. Space Satellite GIF by\nNASA - Find & Share on\nGIPHY. Accessed February 4, 2024. https://giphy.com/gifs/nasa-space-nasagif-3ohc0PkM8mVYfmBHz2.\n\n\n“Google Timelapse.” n.d. Accessed March 9,\n2024. https://earthengine.google.com/timelapse/.\n\n\nHassan, Fawzy, Gouda Salama, Esam Hamza, and H. Hussien. 2006.\n“GEOMETRIC CORRECTION OF REMOTE SENSING SATELLITE DIGITAL\nIMAGES USING MAPPING POLYNOMIAL OF DIFFERENT ORDERS.”\nThe International Conference on Electrical Engineering 5 (5):\n1–22. https://doi.org/10.21608/iceeng.2006.33672.\n\n\nMatin, Shafique, Mukunda Behera, and Surya Mohapatra. 2012.\n“Criteria-Based Approach to Suggest Alternative Road Network in\nPart of Ladakh Province Through Object Based Modelling:\nA GIS & Remote Sensing Method.” Journal of\nEnvironmental Research and Development 7 (January).\n\n\n“Remote Sensing By Satellite: Physical\nBasis, Principles, & Uses.”\n2023. April 7, 2023. https://eos.com/blog/remote-sensing/.\n\n\n“The Nature Conservancy.” n.d. Accessed\nFebruary 4, 2024. https://reefresilience.org/management-strategies/remote-sensing-and-mapping/introduction-to-remote-sensing/what-is-remote-sensing/."
  },
  {
    "objectID": "week_1.html",
    "href": "week_1.html",
    "title": "1  Week 1 - Basics",
    "section": "",
    "text": "2 Summary\nRemote sensing data finds numerous applications across diverse fields. It is used in agriculture for optimizing uses of fertilizers, prediction of crop yields,measuring soil moisture content, monitoring droughts, assessment of crop health, etc.\nSimilarly, in forestry it can be used for monitoring forest cover, tracking deforestation & controlling forest fires.Also, remote sensing is being used for checking the rapid urbanization, planning road networks, pre and post disaster preparedness & in many more fields.\nAlso, during my research regarding the use of remote sensing, I came across one of the works which discussed the application of remote sensing in route planning for road projects in challenging terrain (Mishra (2015)). This study delved into the effectiveness of remote sensing and Geographic Information Systems (GIS) in mapping road infrastructure in rugged landscapes. Various factors influencing road network construction and the resolution of associated challenges were examined within a specific context. Criteria encompassing major and subsidiary factors were assessed to select an appropriate model for comparing attributes and grading them based on their significance. All these examples together demonstrate how valuable remote sensing technology is across a wide range of fields.\nTransitioning from a Civil Engineering background to the realm of spatial science, particularly remote sensing, has been a thrilling journey. One notable application that resonates with me, especially concerning India, is the utilization of remote sensing for identifying “Unconnected Habitations.” These are isolated dwellings or small clusters of houses situated in regions lacking road access, electricity, and water supply. Typically nestled in mountainous terrains, these areas are predominantly inhabited by tribal communities. So, by leveraging remote sensing technology we could identify these remote settlements, facilitating the formulation of targeted policies aimed at fostering development within these underserved regions."
  },
  {
    "objectID": "week_1.html#what-is-remote-sensing-you-ask",
    "href": "week_1.html#what-is-remote-sensing-you-ask",
    "title": "1  Week 1 - Basics",
    "section": "2.1 What is Remote Sensing, you ask?",
    "text": "2.1 What is Remote Sensing, you ask?\nRemote sensing is like Earth’s Personal Observer, capturing information from afar. NASA defines Remote Sensing as acquiring information from a distance - Earthdata-Website (2019) . Also, Elachi and van Zyl (2006) define remote sensing as “the acquisition of information about an object without being in physical contact with it”. All this is achieved through sensors mounted on satellites, planes, drones, etc.\n\n\n\nSpace Satellite GIF by NASA - GIPHY (n.d.)"
  },
  {
    "objectID": "week_1.html#types-of-sensors",
    "href": "week_1.html#types-of-sensors",
    "title": "1  Week 1 - Basics",
    "section": "2.2 Types of Sensors ?",
    "text": "2.2 Types of Sensors ?\nRemote sensing employs two main types of sensors: Passive and Active.\nPassive sensors rely on sunlight reflected off the Earth’s surface. However, they are susceptible to interference from elements like clouds and atmospheric haze.\nActive sensors, on the other hand, emit signals directed towards the Earth, which bounce back to the satellite sensor. This allows active sensors to operate effectively at night and even penetrate through cloud cover, enhancing their versatility.\nThe choice between passive and active sensors depends on the specific environmental conditions and the type of data required for remote sensing applications. A basic difference of the two is shown in the below image explicitly\n\n\n\nDifference between passive and active sensors for remote sensing. Image © “The Nature Conservancy” (n.d.)"
  },
  {
    "objectID": "week_1.html#electromagnetic-spectrum",
    "href": "week_1.html#electromagnetic-spectrum",
    "title": "1  Week 1 - Basics",
    "section": "2.3 Electromagnetic Spectrum ?",
    "text": "2.3 Electromagnetic Spectrum ?\nRemote sensing is based on the principle that there is always an interaction between electromagnetic radiation and an object.Electromagnetic waves travel through the air and space, each with different wavelengths and frequencies. Some waves, like radio and infrared, have long wavelengths, while others, such as ultraviolet and x-rays, have short wavelength. Human eyes can only see a small part called visible light.\nDifferent types of radiation operate in various parts of the electromagnetic spectrum. Sensors can read different wavelengths, thus providing diverse information. Earth’s atmosphere blocks most wavelengths, allowing only radio waves, visible light, and some infrared. Instruments, like passive sensors in the optical window and active sensors using radio waves, help us understand our surroundings.\n\n\n\nElectromagnetic Spectrum, Source Earthdata-Website (2019)"
  },
  {
    "objectID": "week_1.html#resolutions",
    "href": "week_1.html#resolutions",
    "title": "1  Week 1 - Basics",
    "section": "2.4 Resolutions",
    "text": "2.4 Resolutions\nRemote sensing data encompasses four key resolutions:\n\nSpectral Resolution: It refers to the size of each raster cell or pixel, dictating the level of detail captured.\nSpatial Resolution: It defines defines the sensor’s capability to differentiate wavelengths, determining the number of bands recorded, with multispectral sensors typically capturing 3-15 bands and hyperspectral sensors capable of thousands.\nTemporal Resolution:It indicates the frequency at which an area is revisited, influencing the timeliness of data acquisition.\nRadiometric Resolution: It quantifies the information contained within each raster cell, with higher resolutions yielding greater sensitivity to variations.\n\n\n\n\nDiagram showing different Resolutions, Source Darbari and Kumar (2022)"
  },
  {
    "objectID": "week_3.html",
    "href": "week_3.html",
    "title": "3  Week 3 - Corrections",
    "section": "",
    "text": "4 Summary\nThis week’s session focused on rectifying raw satellite images, merging them, and enhancing their quality, particularly delving into the significance of atmospheric corrections in satellite image studies. It’s crucial to recognize that satellite images often suffer from distortions due to various factors which were discussed above, and these can compromise the accuracy of our analysis. To mitigate these distortions, the corrections are routinely applied as a prerequisite before conducting analysis. These corrections ensure that the images are properly aligned and scaled, laying a reliable groundwork for subsequent interpretation and extraction of meaningful data.\nI stumbled upon a research paper by Hassan et al. (2006), where they delve into examining how changes in the number of Ground Control Points (GCPs) and the degree of mapping polynomials affect the accuracy of the geometric correction process. They utilized the Root Mean Square Error (RMS) at the chosen GCPs to gauge the accuracy of their findings. The study revealed that employing a higher degree of polynomial rectification alongside an increased number of GCPs resulted in elevated RMS errors.\nSimilarly, the same image was corrected by using 16 Ground Control Points as shown below.\nThe study revealed that employing a higher degree of polynomial rectification alongside an increased number of GCPs resulted in elevated RMS errors.The results of the study with respect to GCP, RMS is shown below.\nAs I joined the Term quite late, so had to catch up with all in a short amount of time. But, as the lectures and the practical notebooks were crafted perfectly, so I was able to grasp most of the concepts. Regarding, this week, I understand the importance of covering corrections, despite their initial complexity, to ensure data quality and facilitate comparison between different images. Exploring how remotely sensed images can be manipulated reveals deeper layers of information beyond the surface data. Although I didn’t do the practical exercises this week, but I have gone through the code and have also checked the Ahmad (n.d.) as recommended & I found it valuable and I anticipate revisiting it in future."
  },
  {
    "objectID": "week_3.html#corrections",
    "href": "week_3.html#corrections",
    "title": "3  Week 3 - Corrections",
    "section": "4.1 Corrections",
    "text": "4.1 Corrections\nRemotely sensed data requires corrections to remove flaws which may be due to various factors like - atmosphere, sensor sensitivity, illumination conditions, distortions due to viewing angle,topographic conditions,etc.\n- Atmospheric Correction: It removes flaws caused in the remotely sensed data due to atmosphere.The atmosphere affects the incoming sunlight thereby leading to reflection, scattering and absorption of various parts of electromagnetic spectrum. Atmospheric correction is either relative or absolute. Relative atmospheric correction adjusts the data based on radiance values of target area with respect to neighboring area, while as, in absolute atmospheric correction we convert digital brightness values into scaled surface reflectance. This allows us to directly compare these scaled surface reflectance values across different regions of the planet, facilitating accurate analysis and interpretation of Earth’s surface characteristics and changes over time.\n- Geometric Correction: Geometric correction involves adjusting remotely sensed data to account for image distortions introduced by various factors such as the view angle (off-nadir), topography (e.g., non-flat terrain), wind (in aerial acquisitions), and the rotation of the Earth (in satellite imagery). This correction ensures that the satellite image aligns accurately with a coordinate reference system.\n- Topographic Correction: It deals with the effect of terrain variations on reflectance values.It requires sensor geometry and elevation data. After incorporating it, the accuracy and consistency of remote sensing data improves across different terrain conditions.\n- Radiometric Correction: This correction accounts for sensor-related factors such as variations in sensor sensitivity, illumination conditions, and atmospheric effects, ensuring that the resulting data accurately represents the physical properties of the observed features."
  },
  {
    "objectID": "week_5.html",
    "href": "week_5.html",
    "title": "5  Week 5 - Google Earth Engine",
    "section": "",
    "text": "6 Summary\nThis week was for Google Earth Engine (GEE), a cloud-based platform for accessing and processing large geospatial datasets.\n\nWhat is GEE?\n\nGEE is a cloud-based platform for processing large geospatial datasets.\nIt simplifies data access and analysis for researchers, policymakers, NGOs, and the public.\nGEE uses an API and web-based IDE for rapid prototyping and visualization.\nKey terminology includes objects (vectors, rasters, features), images (raster data), and geometries (points, lines, polygons).\n\nFunctionalities\n\nGEE facilitates spatial operations, machine learning tasks, and statistical analyses.\nIts functionalities include reducing imagery, accessing data from multiple sensors, and performing joins and intersections.\n\n\nLast year, shortly after arriving in London, one of my initial tasks was to explore the evolution of Canary Wharf from its roots as docks to its current status as a financial hub. I connected to the city a lot more after this exposure of the history. Let us look at the transformation of Canary Wharf from its origins as dockyards to becoming a bustling business district.You can witness this evolution through Google Timelapse “Google Timelapse” (n.d.), one of the products of GEE. Simply press play button to observe the journey through time\n\n\n\n\n7 Applications\nThere have been so advancment in remote sensing research and ease of doing research with satellite data because of GEE. So, I made a tree map of “What is the craze about GEE and which sectors has it impacted”.\n\n\n\nGEE the bridge that connects, Source - Author\n\n\n\n\n8 Reflections\nGEE presents an intriguing tool for analysis, showcasing the impressive strides in technology and data analysis. Its ability to swiftly process vast amounts of data quickly as compared to tools like SNAP and R, make it exceptionally useful and futuristic. The versatility of GEE is evident in its extensive data access and diverse range of applications, spanning from straightforward to intricate processing tasks. I firmly believe that GEE and cloud computing will revolutionize remote sensing research. Despite my prior experience in Python and R, exploring a new coding language like JavaScript piques my interest, prompting me to consider further learning opportunities.But, again, this entire course of MSc Urban Spatial Science has been an opportunity to go to uncharted territories and moving away from your comfort zone. And despite the demanding nature of the course, this exploration has been immensely enriching, largely thanks to the outstanding faculty at CASA, whom I consider are among the finest at Bartlett.\n\n\n\n\n“Google Timelapse.” n.d. Accessed March 9, 2024. https://earthengine.google.com/timelapse/."
  },
  {
    "objectID": "week_2.html",
    "href": "week_2.html",
    "title": "2  Week 2 - Sensor Presentation",
    "section": "",
    "text": "Burhan"
  },
  {
    "objectID": "week_4.html",
    "href": "week_4.html",
    "title": "4  Week 4 - Policy",
    "section": "",
    "text": "5 Summary\nThis week, we delved into Google Earth Engine (GEE), a cloud-based platform for accessing and processing large geospatial datasets. GEE simplifies high-performance computing for researchers, facilitating sharing of results with various stakeholders. It offers an API and web-based IDE for quick prototyping and visualization. Instead of downloading heavy satellite imagery, GEE can swiftly load datasets, including various types of satellite imagery, via its Data Catalog. GEE operates on client-side code execution, enhancing speed, and utilizes JavaScript. It distinguishes between raster data (images) and vector data (features), organizing them into Image Collections and Feature Collections, respectively."
  },
  {
    "objectID": "week_6.html",
    "href": "week_6.html",
    "title": "6  Week 6 - Classification",
    "section": "",
    "text": "7 Summary\nClassification in remote sensing involves categorizing pixels in imagery to label them based on land cover use or other characteristics. This process is integral to various data processing pipelines and is commonly used for identifying urban areas, vegetation, and other land uses. This week’s lecture focused on classification in remote sensing and its implementation using machine learning techniques.Machine learning methods discussed include classification and regression trees (CART), random forests, maximum likelihood, and support vector machine (SVM). The lecture also covered supervised and unsupervised image classification approaches. Each classification method has its own principles and considerations, such as the need for human knowledge in expert systems, the use of decision rules in maximum likelihood classification, and the margin optimization in SVM. Considerations include whether to classify pixels or objects, the selection of the appropriate machine learning model, and the determination of necessary hyperparameters. Overall, this week’s lecture provided a comprehensive overview of classification methods in remote sensing and their application through machine learning."
  },
  {
    "objectID": "week_1.html#summary",
    "href": "week_1.html#summary",
    "title": "1  Week 1 - Basics",
    "section": "1.1 Summary",
    "text": "1.1 Summary\n\n1.1.1 What is Remote Sensing, you ask?\nRemote sensing is like Earth’s Personal Observer, capturing information from afar. NASA defines Remote Sensing as acquiring information from a distance - Earthdata-Website (2019) . Also, Elachi and van Zyl (2006) define remote sensing as “the acquisition of information about an object without being in physical contact with it”. All this is achieved through sensors mounted on satellites, planes, drones, etc.\n\n\n\nSpace Satellite GIF by NASA - GIPHY (n.d.)\n\n\n\n\n1.1.2 Types of Sensors ?\nRemote sensing employs two main types of sensors: Passive and Active.\nPassive sensors rely on sunlight reflected off the Earth’s surface. However, they are susceptible to interference from elements like clouds and atmospheric haze.\nActive sensors, on the other hand, emit signals directed towards the Earth, which bounce back to the satellite sensor. This allows active sensors to operate effectively at night and even penetrate through cloud cover, enhancing their versatility.\nThe choice between passive and active sensors depends on the specific environmental conditions and the type of data required for remote sensing applications. A basic difference of the two is shown in the below image explicitly\n\n\n\nDifference between passive and active sensors for remote sensing. Image © “The Nature Conservancy” (n.d.)\n\n\n\n\n1.1.3 Electromagnetic Spectrum ?\nRemote sensing is based on the principle that there is always an interaction between electromagnetic radiation and an object.Electromagnetic waves travel through the air and space, each with different wavelengths and frequencies. Some waves, like radio and infrared, have long wavelengths, while others, such as ultraviolet and x-rays, have short wavelength. Human eyes can only see a small part called visible light.\nDifferent types of radiation operate in various parts of the electromagnetic spectrum. Sensors can read different wavelengths, thus providing diverse information. Earth’s atmosphere blocks most wavelengths, allowing only radio waves, visible light, and some infrared. Instruments, like passive sensors in the optical window and active sensors using radio waves, help us understand our surroundings.\n\n\n\nElectromagnetic Spectrum, Source Earthdata-Website (2019)\n\n\n\n\n1.1.4 Resolutions\nRemote sensing data encompasses four key resolutions:\n\nSpectral Resolution: It refers to the size of each raster cell or pixel, dictating the level of detail captured.\nSpatial Resolution: It defines defines the sensor’s capability to differentiate wavelengths, determining the number of bands recorded, with multispectral sensors typically capturing 3-15 bands and hyperspectral sensors capable of thousands.\nTemporal Resolution:It indicates the frequency at which an area is revisited, influencing the timeliness of data acquisition.\nRadiometric Resolution: It quantifies the information contained within each raster cell, with higher resolutions yielding greater sensitivity to variations.\n\n\n\n\nDiagram showing different Resolutions, Source Darbari and Kumar (2022)"
  },
  {
    "objectID": "week_1.html#applications",
    "href": "week_1.html#applications",
    "title": "1  Week 1 - Basics",
    "section": "1.2 Applications",
    "text": "1.2 Applications\nRemote sensing data finds numerous applications across diverse fields. It is used in agriculture for optimizing uses of fertilizers, prediction of crop yields,measuring soil moisture content, monitoring droughts, assessment of crop health, etc.\n\n\n\nA split view comparing NDVI index values on the same field taken 5 days apart. Such a tremendous decrease in NDVI may be the result of water or heat stress, which can be tracked in EOSDA Crop Monitoring., Source\n\n\nSimilarly, in forestry it can be used for monitoring forest cover, tracking deforestation & controlling forest fires.Also, remote sensing is being used for checking the rapid urbanization, planning road networks, pre and post disaster preparedness & in many more fields.\n\n\n\nForest fire damage assessment in the Chillan region of Chile using remote sensing data and EOSDA LandViewer capabilities., Source “Remote Sensing By Satellite: Physical Basis, Principles, & Uses” (2023)\n\n\nAlso, during my research regarding the use of remote sensing, I came across one of the works which discussed the application of remote sensing in route planning for road projects in challenging terrain (Matin, Behera, and Mohapatra (2012)). This study delved into the effectiveness of remote sensing and Geographic Information Systems (GIS) in mapping road infrastructure in rugged landscapes. Various factors influencing road network construction and the resolution of associated challenges were examined within a specific context. Criteria encompassing major and subsidiary factors were assessed to select an appropriate model for comparing attributes and grading them based on their significance. All these examples together demonstrate how valuable remote sensing technology is across a wide range of fields."
  },
  {
    "objectID": "week_1.html#reflections",
    "href": "week_1.html#reflections",
    "title": "1  Week 1 - Basics",
    "section": "1.3 Reflections",
    "text": "1.3 Reflections\nTransitioning from a Civil Engineering background to the realm of spatial science, particularly remote sensing, has been a thrilling journey. One notable application that resonates with me, especially concerning India, is the utilization of remote sensing for identifying “Unconnected Habitations.” These are isolated dwellings or small clusters of houses situated in regions lacking road access, electricity, and water supply. Typically nestled in mountainous terrains, these areas are predominantly inhabited by tribal communities. So, by leveraging remote sensing technology we could identify these remote settlements, facilitating the formulation of targeted policies aimed at fostering development within these underserved regions.\n\n\n\n\nDarbari, Priyanka, and Manoj Kumar. 2022. “Satellite Image Enhancement Techniques: A Comprehensive Review.” In Proceedings of International Conference on Communication and Artificial Intelligence, edited by Vishal Goyal, Manish Gupta, Seyedali Mirjalili, and Aditya Trivedi, 431–47. Lecture Notes in Networks and Systems. Singapore: Springer Nature. https://doi.org/10.1007/978-981-19-0976-4_36.\n\n\nEarthdata-Website. 2019. “What Is Remote Sensing? | Earthdata.” Backgrounder. Earth Science Data Systems, NASA. August 23, 2019. https://www.earthdata.nasa.gov/learn/backgrounders/remote-sensing.\n\n\nElachi, Charles, and van Zyl Jakob J. 2006. “Introduction to the Physics and Techniques of Remote Sensing.” In Introduction to the Physics and Techniques of Remote Sensing, 1–21. Hoboken, NJ, USA: John Wiley & Sons, Inc.\n\n\nGIPHY, dir. n.d. Space Satellite GIF by NASA - Find & Share on GIPHY. Accessed February 4, 2024. https://giphy.com/gifs/nasa-space-nasagif-3ohc0PkM8mVYfmBHz2.\n\n\nMatin, Shafique, Mukunda Behera, and Surya Mohapatra. 2012. “Criteria-Based Approach to Suggest Alternative Road Network in Part of Ladakh Province Through Object Based Modelling: A GIS & Remote Sensing Method.” Journal of Environmental Research and Development 7 (January).\n\n\n“Remote Sensing By Satellite: Physical Basis, Principles, & Uses.” 2023. April 7, 2023. https://eos.com/blog/remote-sensing/.\n\n\n“The Nature Conservancy.” n.d. Accessed February 4, 2024. https://reefresilience.org/management-strategies/remote-sensing-and-mapping/introduction-to-remote-sensing/what-is-remote-sensing/."
  },
  {
    "objectID": "week_4.html#summary",
    "href": "week_4.html#summary",
    "title": "4  Week 4 - Policy",
    "section": "4.1 Summary",
    "text": "4.1 Summary\nThis week, we delved into Google Earth Engine (GEE), a cloud-based platform for accessing and processing large geospatial datasets. GEE simplifies high-performance computing for researchers, facilitating sharing of results with various stakeholders. It offers an API and web-based IDE for quick prototyping and visualization. Instead of downloading heavy satellite imagery, GEE can swiftly load datasets, including various types of satellite imagery, via its Data Catalog. GEE operates on client-side code execution, enhancing speed, and utilizes JavaScript. It distinguishes between raster data (images) and vector data (features), organizing them into Image Collections and Feature Collections, respectively."
  },
  {
    "objectID": "week_5.html#summary",
    "href": "week_5.html#summary",
    "title": "5  Week 5 - Google Earth Engine",
    "section": "5.1 Summary",
    "text": "5.1 Summary\nThis week was for Google Earth Engine (GEE), a cloud-based platform for accessing and processing large geospatial datasets.\n\nWhat is GEE?\n\nGEE is a cloud-based platform for processing large geospatial datasets.\nIt simplifies data access and analysis for researchers, policymakers, NGOs, and the public.\nGEE uses an API and web-based IDE for rapid prototyping and visualization.\nKey terminology includes objects (vectors, rasters, features), images (raster data), and geometries (points, lines, polygons).\n\nFunctionalities\n\nGEE facilitates spatial operations, machine learning tasks, and statistical analyses.\nIts functionalities include reducing imagery, accessing data from multiple sensors, and performing joins and intersections.\n\n\nLast year, shortly after arriving in London, one of my initial tasks was to explore the evolution of Canary Wharf from its roots as docks to its current status as a financial hub. I connected to the city a lot more after this exposure of the history. Let us look at the transformation of Canary Wharf from its origins as dockyards to becoming a bustling business district.You can witness this evolution through Google Timelapse “Google Timelapse” (n.d.), one of the products of GEE. Simply press play button to observe the journey through time"
  },
  {
    "objectID": "week_5.html#applications",
    "href": "week_5.html#applications",
    "title": "5  Week 5 - Google Earth Engine",
    "section": "5.2 Applications",
    "text": "5.2 Applications\nThere have been so advancment in remote sensing research and ease of doing research with satellite data because of GEE. So, I made a tree map of “What is the craze about GEE and which sectors has it impacted”.\n\n\n\nGEE the bridge that connects, Source - Author"
  },
  {
    "objectID": "week_5.html#reflections",
    "href": "week_5.html#reflections",
    "title": "5  Week 5 - Google Earth Engine",
    "section": "5.3 Reflections",
    "text": "5.3 Reflections\nGEE presents an intriguing tool for analysis, showcasing the impressive strides in technology and data analysis. Its ability to swiftly process vast amounts of data quickly as compared to tools like SNAP and R, make it exceptionally useful and futuristic. The versatility of GEE is evident in its extensive data access and diverse range of applications, spanning from straightforward to intricate processing tasks. I firmly believe that GEE and cloud computing will revolutionize remote sensing research. Despite my prior experience in Python and R, exploring a new coding language like JavaScript piques my interest, prompting me to consider further learning opportunities.But, again, this entire course of MSc Urban Spatial Science has been an opportunity to go to uncharted territories and moving away from your comfort zone. And despite the demanding nature of the course, this exploration has been immensely enriching, largely thanks to the outstanding faculty at CASA, whom I consider are among the finest at Bartlett.\n\n\n\n\n“Google Timelapse.” n.d. Accessed March 9, 2024. https://earthengine.google.com/timelapse/."
  },
  {
    "objectID": "week_3.html#applications",
    "href": "week_3.html#applications",
    "title": "3  Week 3 - Corrections",
    "section": "3.2 Applications",
    "text": "3.2 Applications\nThis week’s session focused on rectifying raw satellite images, merging them, and enhancing their quality, particularly delving into the significance of atmospheric corrections in satellite image studies. It’s crucial to recognize that satellite images often suffer from distortions due to various factors which were discussed above, and these can compromise the accuracy of our analysis. To mitigate these distortions, the corrections are routinely applied as a prerequisite before conducting analysis. These corrections ensure that the images are properly aligned and scaled, laying a reliable groundwork for subsequent interpretation and extraction of meaningful data.\nI stumbled upon a research paper by Hassan et al. (2006), where they delve into examining how changes in the number of Ground Control Points (GCPs) and the degree of mapping polynomials affect the accuracy of the geometric correction process. They utilized the Root Mean Square Error (RMS) at the chosen GCPs to gauge the accuracy of their findings. The study revealed that employing a higher degree of polynomial rectification alongside an increased number of GCPs resulted in elevated RMS errors.\n\n\n\na & b – Shows the raw image2 and its corresponding reference image, c & d – 4 GCPs distribution on raw image2 and its reference, e & f – corrected image2, and its reference,Source:Hassan et al. (2006)\n\n\nSimilarly, the same image was corrected by using 16 Ground Control Points as shown below.\n\n\n\na & b – shows the raw image2 and its corresponding reference image, c & d – 16 GCPs distribution on raw image2 and its reference, e & f – corrected image2, and its reference, Source Hassan et al. (2006)\n\n\nThe study revealed that employing a higher degree of polynomial rectification alongside an increased number of GCPs resulted in elevated RMS errors.The results of the study with respect to GCP, RMS is shown below.\n\n\n\nAverage of RMS error of all test images at each order of polynomials, Source Hassan et al. (2006)"
  },
  {
    "objectID": "week_3.html#reflections",
    "href": "week_3.html#reflections",
    "title": "3  Week 3 - Corrections",
    "section": "3.3 Reflections",
    "text": "3.3 Reflections\nAs I joined the Term quite late, so had to catch up with all in a short amount of time. But, as the lectures and the practical notebooks were crafted perfectly, so I was able to grasp most of the concepts. Regarding, this week, I understand the importance of covering corrections, despite their initial complexity, to ensure data quality and facilitate comparison between different images. Exploring how remotely sensed images can be manipulated reveals deeper layers of information beyond the surface data. Although I didn’t do the practical exercises this week, but I have gone through the code and have also checked the Ahmad (n.d.) as recommended & I found it valuable and I anticipate revisiting it in future.\n\n\n\n\nAhmad, Dr. Zia. n.d. “Geospatial Data Science in R.” Accessed March 9, 2024. https://zia207.github.io/geospatial-r-github.io/index.html.\n\n\nHassan, Fawzy, Gouda Salama, Esam Hamza, and H. Hussien. 2006. “GEOMETRIC CORRECTION OF REMOTE SENSING SATELLITE DIGITAL IMAGES USING MAPPING POLYNOMIAL OF DIFFERENT ORDERS.” The International Conference on Electrical Engineering 5 (5): 1–22. https://doi.org/10.21608/iceeng.2006.33672."
  },
  {
    "objectID": "week_3.html#summary",
    "href": "week_3.html#summary",
    "title": "3  Week 3 - Corrections",
    "section": "3.1 Summary",
    "text": "3.1 Summary\n\n3.1.1 Corrections\nRemotely sensed data requires corrections to remove flaws which may be due to various factors like - atmosphere, sensor sensitivity, illumination conditions, distortions due to viewing angle,topographic conditions,etc.\n- Atmospheric Correction: It removes flaws caused in the remotely sensed data due to atmosphere.The atmosphere affects the incoming sunlight thereby leading to reflection, scattering and absorption of various parts of electromagnetic spectrum. Atmospheric correction is either relative or absolute. Relative atmospheric correction adjusts the data based on radiance values of target area with respect to neighboring area, while as, in absolute atmospheric correction we convert digital brightness values into scaled surface reflectance. This allows us to directly compare these scaled surface reflectance values across different regions of the planet, facilitating accurate analysis and interpretation of Earth’s surface characteristics and changes over time.\n- Geometric Correction: Geometric correction involves adjusting remotely sensed data to account for image distortions introduced by various factors such as the view angle (off-nadir), topography (e.g., non-flat terrain), wind (in aerial acquisitions), and the rotation of the Earth (in satellite imagery). This correction ensures that the satellite image aligns accurately with a coordinate reference system.\n- Topographic Correction: It deals with the effect of terrain variations on reflectance values.It requires sensor geometry and elevation data. After incorporating it, the accuracy and consistency of remote sensing data improves across different terrain conditions.\n- Radiometric Correction: This correction accounts for sensor-related factors such as variations in sensor sensitivity, illumination conditions, and atmospheric effects, ensuring that the resulting data accurately represents the physical properties of the observed features."
  },
  {
    "objectID": "week_6.html#summary",
    "href": "week_6.html#summary",
    "title": "6  Week 6 - Classification",
    "section": "6.1 Summary",
    "text": "6.1 Summary\nClassification in remote sensing involves categorizing pixels in imagery to label them based on land cover use or other characteristics. This process is integral to various data processing pipelines and is commonly used for identifying urban areas, vegetation, and other land uses. This week’s lecture focused on classification in remote sensing and its implementation using machine learning techniques.Machine learning methods discussed include classification and regression trees (CART), random forests, maximum likelihood, and support vector machine (SVM). The lecture also covered supervised and unsupervised image classification approaches. Each classification method has its own principles and considerations, such as the need for human knowledge in expert systems, the use of decision rules in maximum likelihood classification, and the margin optimization in SVM. Considerations include whether to classify pixels or objects, the selection of the appropriate machine learning model, and the determination of necessary hyperparameters. Overall, this week’s lecture provided a comprehensive overview of classification methods in remote sensing and their application through machine learning."
  },
  {
    "objectID": "week_7.html#summary",
    "href": "week_7.html#summary",
    "title": "7  Week 7 - Classification I",
    "section": "7.1 Summary",
    "text": "7.1 Summary\nClassification in remote sensing is the process of sorting pixels in images to label them, which is essential for tasks such as monitoring urban green spaces or combating illegal logging in forests, and other land uses. This week’s lecture focused on classification in remote sensing and its implementation using machine learning techniques.Machine learning methods discussed include classification and regression trees (CART), random forests, maximum likelihood, and support vector machine (SVM). The lecture also covered supervised and unsupervised image classification approaches. Each classification method has its own principles and considerations, such as the need for human knowledge in expert systems, the use of decision rules in maximum likelihood classification, and the margin optimization in SVM. Considerations include whether to classify pixels or objects, the selection of the appropriate machine learning model, and the determination of necessary hyperparameters.\n\n7.1.1 Classification and Regression Trees (CART)?\n\nIt creates a hierarchical model composed of nodes and branches. Nodes signify decision points, while branches represent outcomes. At the end of the hierarchy, leaf nodes contain predicted values for the target variable.\nIt creates a hierarchical model composed of nodes and branches. Nodes signify decision points, while branches represent outcomes. At the end of the hierarchy, leaf nodes contain predicted values for the target variable.\n\nYAY’S\n\nClassification and regression trees implicitly perform feature selection.\nOutliers have no meaningful effect on CART.\nIt requires minimal supervision and produces easy-to-understand models.\n\nNAY’S\n\nOverfitting.\nHigh Variance.\nLow bias.\nStructure may be unstable.\n\n\n\n7.1.2 Random Forest?\n\nRandom forests use many decision trees together. Each tree is made from a different part of the data, and their predictions are combined to give a final answer. This helps overcome any mistakes from individual trees, making the overall result more accurate.\nRandom forests are better than single decision trees because they are more flexible, accurate, and easier to use. They’re great for both classifying things and predicting values, making them a top pick in machine learning. Basically, random forests make decision trees work better by teaming them up, resulting in better performance overall.\n\nYAY’S\n\nProvide high accuracy in both classification and regression tasks.\nRobust to overfitting\nSuitable for a wide range of tasks, including handling both numerical and categorical data.\n\nNAY’S\n\nRandom forests can be more complex to interpret\nTraining a random forest can be computationally expensive and require significant memory.\nOptimizing hyperparameters for random forests can be challenging and time-consuming.\nMay not perform well with very small datasets due to the risk of overfitting and limited diversity among trees."
  },
  {
    "objectID": "week_7.html#applications",
    "href": "week_7.html#applications",
    "title": "7  Week 7 - Classification I",
    "section": "7.2 Applications",
    "text": "7.2 Applications"
  },
  {
    "objectID": "week_7.html#reflections",
    "href": "week_7.html#reflections",
    "title": "7  Week 7 - Classification I",
    "section": "7.3 Reflections",
    "text": "7.3 Reflections\nThis week was a lot of important topics and it explored the intricacies of machine learning for informed decision-making, even if the technical details are difficult to grasp. I will be revising this lecture and will be using these techniques in Google Earth Engine in near future."
  },
  {
    "objectID": "week_8.html#summary",
    "href": "week_8.html#summary",
    "title": "8  Week 8 - Classification II",
    "section": "8.1 Summary",
    "text": "8.1 Summary\nThis week’s focus remained on exploring additional classification methods with particular attention given to object-based image analysis and sub-pixel analysis. Below, I will explore these in details below.\n\n8.1.1 Object Based Image Analysis (OBIA)\nIn contrast to pixel-based classification methods that classify individual pixels directly, object-based classification first aggregates image pixels into spectrally homo-genous image objects using an image segmentation algorithm which clusters pixels based on spatial proximity and colour similarity and resulting in classification of individual objects (Liu and Xia (2010)).\n\n\n8.1.2 Sub-Pixel Analysis\nSub-pixel analysis examines the contents of a single pixel, determining the mix of land cover types within it. It calculates the percentage of each land cover type per pixel by comparing reflectance values.This method helps classify pixels, particularly when they contain multiple land cover types. It estimates the portion of each pixel covered by different materials. Unlike multi-pixel analysis, sub-pixel analysis concentrates on individual pixels, essential for satellite images with low resolution where each pixel covers a larger land area.\nThe study by Whiteside, Boggs, and Maier (2011) explains the object-based & pixel type of analysis.In the pixel-based classification, each pixel is classified separately, whereas in the object-based classification, all pixels that belong to one GIS object are grouped together. The result of the pixel grouping is like a smoothing of the data. Resultant noise in the pixel-based classification shows why OBIA has great advantage over it.\n\n\n\nLST Values for different years,of Indianapolis, USA. Source - Whiteside, Boggs, and Maier (2011)\n\n\n\n\n8.1.3 Accuracy\nIn remote sensing and machine learning, we use different methods to check how accurate our models are. The one we pick depends on what we’re using the model for. Some common ones are user’s accuracy, producer’s accuracy, and F1 score which we obtain from confusion matrix - which is a table showing how many pixels were classified correctly or incorrectly for each class. Here is a brief overview of all these parameters\n\n\n\n\n\n\n\n\n\n\n\nProducer’s Accuracy\nUser’s Accuracy\nOverall Accuracy\nKappa Coefficient\n\n\n\n\nExplanation\nMeasures the proportion of actual positive cases that were correctly identified positive cases.\nMeasures the proportion of correctly identified positive cases out of all cases classified as positive.\nMeasures the proportion of all correctly classified pixels out of all observations.\nMeasures the agreement between observed and expected accuracy levels when there is no agreement by chance.\n\n\nFormula\n\\(\\frac{TP}{TP+FN}\\)\n\\(\\frac{TP}{TP+FP}\\)\n\\(\\frac{TP+TN}{TP+FP+TN+FN}\\)\n\\(\\frac{p_o-p_e}{1-p_e}\\)"
  },
  {
    "objectID": "week_8.html#applications",
    "href": "week_8.html#applications",
    "title": "8  Week 8 - Classification II",
    "section": "8.2 Applications",
    "text": "8.2 Applications\nOne of the studies that came across while reading about Sub-Pixel Analyis was of Weng and Lu (2008) in which they studied the urbanization effect on land surface temperature (LST) in USA. In this study they gathered the satellite images from different time intervals (in years) and they analyzed the rise in urbanization with the land surface temperature. The study demonstrated that Urbanization created an evolved inverse relationship between impervious and vegetation coverage, and brought about new LST patterns because of LST’s correlations with both impervious and vegetation coverage.\n\n\n\nLST Values for different years,of Indianapolis, USA. Source - Weng and Lu (2008)\n\n\nAlso, one of the application of Object-based image analysis came across during my reading was the study done by Najafi et al. (2018) in Iran to determine the crop residue estimate using Landsat data by applying the OBIA. To process the data, they applied the object-based image processing steps which include segmentation and classification and developed intelligent objects. Further, after applying OBIA algorithms results were validated against ground control data set by field survey.\n\n\n\nClassification results of tillage indices functions: a to f. Source - Najafi et al. (2018)"
  },
  {
    "objectID": "week_8.html#reflections",
    "href": "week_8.html#reflections",
    "title": "8  Week 8 - Classification II",
    "section": "8.3 Reflections",
    "text": "8.3 Reflections\nThis week touched on the disparity between real-world features and satellite data, underscoring the importance of employing classification methods such as subpixel and object-based approaches to accurately interpret the data. Additionally, it’s crucial to refrain from depending solely on commonly used metrics without verifying their validity. We need to assess whether the coefficients we utilize are suitable for the specific task at hand. Also, the impact of spatial autocorrelation in classification systems offers a fresh perspective, particularly when dealing with other spatial data. Reflecting on all of this, I feel content and pleased to know the practical applications of remote sensing data, particularly in urban environments.\n\n\n\n\nLiu, Desheng, and Fan Xia. 2010. “Assessing Object-Based Classification: Advantages and Limitations.” Remote Sensing Letters 1 (4): 187–94. https://doi.org/10.1080/01431161003743173.\n\n\nNajafi, P., H. Navid, B. Feizizadeh, and I. Eskandari. 2018. “Object-Based Satellite Image Analysis Applied for Crop Residue Estimating Using Landsat OLI Imagery.” International Journal of Remote Sensing 39 (19): 6117–36. https://doi.org/10.1080/01431161.2018.1454621.\n\n\nWeng, Qihao, and Dengsheng Lu. 2008. “A Sub-Pixel Analysis of Urbanization Effect on Land Surface Temperature and Its Interplay with Impervious Surface and Vegetation Coverage in Indianapolis, United States.” International Journal of Applied Earth Observation and Geoinformation 10 (1): 68–83. https://doi.org/10.1016/j.jag.2007.05.002.\n\n\nWhiteside, Timothy G., Guy S. Boggs, and Stefan W. Maier. 2011. “Comparing Object-Based and Pixel-Based Classifications for Mapping Savannas.” International Journal of Applied Earth Observation and Geoinformation 13 (6): 884–93. https://doi.org/10.1016/j.jag.2011.06.008."
  },
  {
    "objectID": "week_9.html#summary",
    "href": "week_9.html#summary",
    "title": "9  Week 9 - SAR",
    "section": "9.1 Summary",
    "text": "9.1 Summary"
  },
  {
    "objectID": "week_9.html#applications",
    "href": "week_9.html#applications",
    "title": "9  Week 9 - SAR",
    "section": "9.2 Applications",
    "text": "9.2 Applications"
  },
  {
    "objectID": "week_9.html#reflections",
    "href": "week_9.html#reflections",
    "title": "9  Week 9 - SAR",
    "section": "9.3 Reflections",
    "text": "9.3 Reflections"
  }
]